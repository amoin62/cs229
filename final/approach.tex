\section{Content Classification Approach}
\label{sec:approach}
We used $4$ supervised learning methods for the problems of sentiment and category classification.
In both cases, we deal with multi-class classification problems. In sentiment analysis, there are $3$ classes corresponding to negative, neutral and positive sentiments while in category classification there are $15$ classes corresponding to different categories.
Theses techniques include Logistic Regression, Ridge classifier, Bernoulli Naive Bayes classifier and linear Support Vector Machine classifier.

Bernoulli Naïve Bayes model is considered a common baseline for text classification due to its speed.
This model assumes conditional independence between the features given their class. This assumption leads to relatively straightforward analysis and efficient running time. Nevertheless, the performance is less desirable than other methods because the independence condition is usually not satisfied in real-world problems.

Bernoulli Naïve Bayes model estimates the classes probabilities as follows.
Given $x^{(i)}=(x_1^{(i)},x_2^{(i)}.....x_j^{(i)})$ being the $i^{\mathrm{th}}$ training example and $K$ classes, we first maximize the joint likelihood of the data:
\begin{eqnarray*}
l(\phi_y,\phi_{j|y=1},...,\phi_{j|y=k}) = \prod_{i = 1}^{m} \prod_{j = 1}^{n} p(x_j^{(i)}|y^{(i)})p(y^{(i)}).
\end{eqnarray*}
Maximizing the joint likelihood of the training data yields:
\begin{eqnarray*}
&&p(x_j^{(i)}|y^{(i)}=k) = \phi_{j|y=k}=\frac{\sum_{i = 1}^{m} 1 \{x_j^{(i)} = 1 \: \Lambda \: y^{(i)}=k]}{\sum_{i = 1}^{m}1[y^{(i)}=k\}}, \\
&&p(y^{(i)}=k) = \phi_{y=k} = \frac{\sum_{i = 1}^{m} 1\{y^{(i)}=k\}}{m},
\end{eqnarray*}
where $m$ is the number of training examples.
Then, class label is chosen as the one with the highest posterior probability:
\begin{equation*}
y = \mathrm{argmax}_k \frac{\phi_{y=k} \prod_{j = 1}^{n} \phi_{j|y}}{\sum_{k = 1}^{K} \phi_{y=k} \prod_{j = 1}^{n} \phi_{j|y}}
\end{equation*}

Multinomial Logistic Regression, also known as Softmax regression is a generalization of Logistic Regression to more than $2$ classes.
Matrix of parameters $\theta$ is found maximizing the log-likelihood function defined as:
\begin{equation*}
	%l(\theta) = \sum_{i = 1}^{m} log \prod_{j = 1}^{n} \frac{exp(\theta_{j}^Tx^{(i)})}{\sum_k exp(\theta_{j}^Tx^{(i)})}
	l(\theta) = \sum_{i=1}^{m} log \prod_{l=1}^K \begin{pmatrix}
\frac{exp(\theta_l^Tx^{(i)})}{\sum_{j=1}^K exp(\theta_j^Tx^{(i)})}
\end{pmatrix}^{1\{y^{(i)}=l\}}
\label{eq:softmax-likelihood}
\end{equation*}
The probability of each data point is then computed as:
\begin{equation*}
 p(y^{(i)}=k|x^{(i)};\theta)=\frac{exp(\theta^T_k x^{(i)})}{\sum_{j = 1}^{n} exp(\theta_j^T x^{(i)})}
\label{eq:softmax-prob}
\end{equation*}

Ridge classification is a generalization of regularized linear regression to classification problems.
Class probabilities are estimated by first minimizing a least squares cost function with L2 norm:
\begin{equation*}
\mathrm{argmin}_{\theta}\sum_{i = 1}^{m} \left\| \theta x^{(i)}-y^{(i)}\right\|_2^2 + \lambda \left\| \theta \right\|_2^2,
\label{eq:ridge-cost}
\end{equation*}
where $\lambda >= 0$ is the regularization constant.
Once parameters are computed, continuous values are estimated for each data point through a linear prediction function $\hat{Y} = X\theta$. 
These values are then converted into discrete class values through proper thresholds.
The second term is used to penalize the matrix $W$ being too large. This will prevent the model from being unnecessarily complicated by overfitting the train data.

Linear Support Vector classifier (LinearSVC) applies a Support Vector Machine (SVM) with a linear Kernel. 
SVM is a maximum-margin classifier seeking to find a hyperplane $W^T X + b$ in a high-dimensional space solving the following \textit{binary} constrained optimization problem.
\begin{eqnarray*}
&&\mathrm{min}_{w_\tau,b_k,\tau_k}(\frac{1}{2}W_k^TW_k +c\sum_{i = 1}^{m} \tau_k^{(i)}), \\
&&\mathrm{s.t.} \quad y^{(i)}(W^T_kx^{(i)}+b_k)\geq 1-\tau_k^{(i)}, \\ 
&&\mathrm{s.t.} \quad 0 \leq \tau_k^{(i)} \leq 1,
\end{eqnarray*}
where $W_k$, $\tau_k$ and $b_k$ are the parameters for the $k$-th SVM classifier corresponding to the $k$-th class.
Support Vector Machines are inherently binary classifiers. To generalize them to the case of a multi-class problem with $K$ classes, there are two possible methods: one-vs-one and one-vs-all. The one-vs-one method builds a classifier for every two classes. The class of one data point is then the one chosen by the most classifiers. The one-vs-all approach builds a classifier for each class against the data points of all the remaining classes. The class of a data point is the one whose classifier achieves the greatest margin. In our experiment, we apply the one-vs-all method.