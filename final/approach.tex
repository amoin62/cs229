\section{Content Classification Approach}
\label{sec:approach}
We used $4$ supervised learning methods for the problems of sentiment and category classification.
In both cases we deal with multi-class classification problems. In sentiment analysis, there are $3$ classes corresponding to negative, neutral and positive sentiments while in category classification there are $15$ classes corresponding to different categories.
Theses techniques include Logistic Regression, Ridge classifier, Bernoulli Naive Bayes classifier and linear Support Vector Machine classifier.

Bernoulli NaÃ¯ve Bayes model is considered a common baseline for text classification due to its speed.
This model assumes conditional independence between the features given their class. This assumption leads to relatively straightforward analysis and efficient running time. Nevertheless, the performance is less desirable than other methods because the independence condition is usually not satisfied in real-world problems.


Given $x^{(i)}=(x_1^{(i)},x_2^{(i)}.....x_j^{(i)})$ being the training example(i) , we first maximize the joint likelihood of the data:
$$l(\phi_y,\phi_{j|y=1},...,\phi_{j|y=k})=\prod_i p(x^{(i)},y^{(i)}) = \prod_i \prod_j p(x_j^{(i)}|y^{(i)}) * p(y^{(i)})$$


Maximizing the joint likelihood of the training data gives :
$$p(x_j^{(i)}|y^{(i)}=k)=\phi_{j|y=k}=\frac{(\sum_i 1 [x_j^{(i)}=1 \Lambda y^{(i)}=k])}{\sum_l1[y^{(i)}=k]}$$
$p(y^{(i)}=k)=\phi_{y=k}=\frac{\sum_l [y^{(i)}=k]}{m} $, where m = number of training examples
Then, class label will be based on the highest posterior probability:
\begin{equation}
y = arg\: max_k \frac{\prod \phi_{j|y}(x_j^{(i)}*\phi_{y=k})}{\sum_k \prod_j \phi_{j|y}(x_j^{(i)}*\phi_{y=k})}
\end{equation}
%assumes a very strong assumption which assumes that the probability of each word occurrence in a comment is conditionally independent of the occurrence of other words in a comment.
%Given ð‘¥ (i) = (ð‘¥1 (i), ð‘¥2 (i), â€¦ ð‘¥j (i)) being the training example (i), we first maximize the joint likelihood of the data:
%L (Ï†y, Ï†j|y=1, â€¦, Ï†j|y=k) = âˆi  p (ð‘¥ (i), y (i) ) = âˆi  âˆj p (ð‘¥j (i)| y(i)) * p (y(i))
%
%Maximizing the joint likelihood of the training data gives:
%p (ð‘¥j (i)| y(i) = k) = Ï†j|y=k = (âˆ‘I 1 {ð‘¥j (i) = 1âˆ§ y(i)=k}) / (âˆ‘I 1 {y(i)=k})
%p (y(i)=k) = Ï†y=k = (âˆ‘I 1 {y(i)=k}) / m, where m = number of training examples.
%
%Then, class label will be based on the highest posterior probability:
%y = argmaxk (âˆj Ï†j|y (ð‘¥ j (i)) * Ï†y=k  ) / (âˆ‘k âˆj Ï†j|y (ð‘¥ j (i)) * Ï†y=k)

Multinomial Logistic Regression, also known as softmax regression is a generalization of Logistic Regression to more than $2$ classes.
Matrix of parameters $\theta$ is found maximizing the log-likelihood function defined as:
\begin{equation}
	l(\theta) = \sum_i log \prod_j \frac{exp(\theta^Tx^{(i)})}{\sum_k exp(\theta^Tx^{(i)})}
\label{eq:softmax-likelihood}
\end{equation}
The probability of each data point is then computed as:
\begin{equation}
 p(y=k|x;\theta)=\frac{exp(\theta^T_k x^{(i)})}{\sum_j exp(\theta_j^T x^{(i)})}
\label{eq:softmax-prob}
\end{equation}

Ridge classification is a generalization of regularized linear regression to classification problems.
The cost function to minimize if least squares error with L2 norm defined as:
\begin{equation}
\sum_i ||x^{(i)}W-y^{(i)}||_2^2 + \alpha ||W||_2^2, \alpha\geq 0
\label{eq:ridge-cost}
\end{equation}
Once parameters are computed, continues predicted values are done according to a linear prediction function $X\theta$. These values are then converted into discrete class values through proper thresholds.


The second term is used to penalize the matrix W being too large. In other words, if the matrix W takes on large values, regularized loss function will be penalized. This will encourage the fitted model to be a simple model rather than a complex model and usually this will prevent overfitting.

Linear Support Vector classifier (LinearSVC) applies a Support Vector Machine (SVM) with a linear Kernel. SVM is a maximum-margin classifier seeking to find a hyperplane $W^T X + b$ in a high-dimensional space that solves the following constrained optimization problem.
SVM has an objective to maximize the margin between the classes. This can be achieve by determining the parameters of the hyperplane (w and b) subject to $w^T x + b = 0$.

%Given ð‘¥ (i) = (ð‘¥1 (i), ð‘¥2 (i), â€¦ ð‘¥j (i)) being the training example (i) and the output y âˆˆ {âˆ’1, 1}, the goal is to %solve the following problematic. For the SVM k, we wish to:
%min (1/2 WkT Wk + c âˆ‘i  Î¶k(i)) , where Wk Î¶k bk are the parameters for kth SVM
%wk,bk,Î¶k
%subject to y(i) (WkT ð‘¥ (i) + bk) â‰¥ 1 - Î¶k(i) and Î¶k(i) â‰¥ 0

Given $x^{(i)}=(x_1^{(i)},x_2^{(i)}.....x_j^{(i)})$ being the training example(i) and the output $y \in {0,1}$, the goal is to solve the following problematic. For the svm k , we wish to :
$$min_{w_\tau,b_k,\tau_k}(\frac{1}{2}W_k^TW_k +c\sum_i \tau_k^{(i)})$$, where $W_k,\tau_k,b_k$ are the parameters for the kth SVM subject to $y^{(i)}(W^T_kx^{(i)}+b_k)\geq 1-\tau_k^{(i)} and \: \tau_k^{(i)}>0$
In our experiment, since we are dealing with multi-class classification, we will employ one-vs.-all fashion so k SVMs is build, where k is the number of class label.

Support Vector Machines are inherently binary classifiers. To generalize them to the case of multi-class problems, there are two possible methods: one-vs-one and one-vs-all. The one-vs-one method builds a classifier for every two classes. The class of a point is then the one chosen by the most classifiers. The one-vs-all approach builds a classifier for each class compared to all the remaining classes. The class of a data point is the one whose classifier achieves the greatest margin. In our experiment, we apply the one-vs-all method.